#include <aten/src/ATen/core/jit_type.h>
#include <c10/core/DeviceType.h>

#include <torch/csrc/jit/fuser/interface.h>

#include <torch/csrc/jit/fuser/compiler.h>
#include <torch/csrc/jit/fuser/executor.h>
#include <torch/csrc/jit/fuser/fallback.h>
#include <torch/csrc/jit/fuser/kernel_cache.h>

#include <asmjit/asmjit.h>

#include <stdexcept>

#define FUSER_DEBUG 1

namespace torch {
namespace jit {

namespace detail {

// Note: CPU fusion is currently disabled due to test flakiness
bool cpu_fuser_enabled = false;

} // namespace detail

// TODO: make dbgs take stream to print on
namespace {

void printScalar(const Value* const value) {
  if (value->node()->kind() == prim::Constant) {
    std::cout << "Const Scalar: ";
  } else {
    std::cout << "Scalar: ";
  }

  if (value->type() == FloatType::get()) {
    std::cout << "float ";
    const float val = value->node()->f(attr::value);
    std::cout << val;
  } else if (value->type() == IntType::get()) {
    std::cout << "int ";
    const int val = value->node()->i(attr::value);
    std::cout << val;
  } else {
    std::cout << "unknown";
  }

  std::cout << std::endl;
}

void printStrides(const c10::VaryingStrides& strides) {
  std::cout << "Strides: ";
  for (size_t i = 0; i < *(strides.size()); ++i) {
    std::cout << *(strides[i]) << " ";
  }

  std::cout << std::endl;
}

void printSizes(const c10::VaryingShape& sizes) {
  std::cout << "Sizes: ";
  for (size_t i = 0; i < *(sizes.size()); ++i) {
    std::cout << *(sizes[i]) << " ";
  }

  std::cout << std::endl;
}

void printCompleteTensor(const std::shared_ptr<c10::TensorType> tensor) {
  std::cout << "Complete Tensor: ";
  std::cout << *(tensor->device()) << " ";
  std::cout << *(tensor->scalarType()) << " ";
  std::cout << "nDims: " << *(tensor->dim()) << " ";
  std::cout << std::endl;
  printSizes(tensor->sizes());
  printStrides(tensor->strides());
}

void printValue(const Value* const value) {
  if (value->isCompleteTensor()) {
    printCompleteTensor(value->type()->expect<TensorType>());
  } else if (value->type()->isSubtypeOf(NumberType::get())) {
    printScalar(value);
  } else {
    std::cout << "Request to print unknown value" << std::endl;
  }
}

// Returns true if and only if value is a scalar or a complete tensor type
bool validateValue(const Value* const value, const bool dbg = false) {
  if (dbg) {
    printValue(value);
  }

  if (value->isCompleteTensor() || value->type()->isSubtypeOf(NumberType::get())) {
    return true;
  }

  return false;
}

// Returns true if all inputs and outputs are complete tensors or scalars
// Note: complete tensor means device, nDims, sizes, and strides are known
// In particular, all optional values of sizes and strides have values
bool validateNode(const Node* const node, const bool dbg = false) {
  auto inputs = node->inputs();
  auto outputs = node->outputs();

  if (dbg) {
    std::cout << "nInputs: " << inputs.size() << std::endl;
    std::cout << "nOutputs: " << outputs.size() << std::endl;
  }

  if (dbg) {
    std::cout << "Inputs: " << std::endl;
  }

  for (auto i = decltype(inputs.size()){0}; i < inputs.size(); ++i) {
    if (!validateValue(inputs[i], dbg)) {
      return false;
    }
  }

  if (dbg) {
    std::cout << "Outputs: " << std::endl;
  }

  for (auto i = decltype(outputs.size()){0}; i < outputs.size(); ++i) {
    if (!validateValue(outputs[i], dbg)) {
      return false;
    }
  }

  return true;
}

} // namespace

// Signature of the generated function.
typedef int (*Func)(void);
typedef void (*addFunc)(unsigned, float*, float*, float*);

using namespace asmjit;
using namespace asmjit::x86;

bool cpuMergeNodeWithFusionGroup(const Node* const node, Node* fusion_group) {
  #if FUSER_DEBUG
    std::cout << "cpuMergeNodeWithFusionGroup" << std::endl;
  #endif // FUSER_DEBUG

  JitRuntime rt;

  CodeHolder code;
  code.init(rt.codeInfo());

  Assembler a(&code);

  // allocates sample arrays
  constexpr unsigned array_size = 32;
  float* out = (float*)malloc(sizeof(float) * array_size);
  float* lhs = (float*)malloc(sizeof(float) * array_size);
  float* rhs = (float*)malloc(sizeof(float) * array_size);

  // initializes sample arrays
  for (auto i = decltype(array_size){0}; i < array_size; ++i) {
    out[i] = 0.f;
    lhs[i] = 1.f;
    rhs[i] = 2.f;
  }

  // Creates fusion (element-by-element contiguous add)
  Label LoopInc = a.newLabel();
  Label LoopBody = a.newLabel();
  Label Exit = a.newLabel();

  // Short-circuits on size == 0
  a.test(edi, edi);
  a.je(Exit);

  // Stores # of loop iterations, sets loop counter to zero
  a.lea(r8d, dword_ptr(rdi, - 1)); // r8 = size - 1
  a.xor_(eax, eax); // clears eax
  a.jmp(LoopBody); // do () { } while () loop form

  // Loop incrementer
  a.bind(LoopInc);
  a.mov(rax, rdi); // offset = offset + 1

  // Loop body
  a.bind(LoopBody);
  a.vmovss(xmm0, dword_ptr(rdx, rax, 2)); // xmm0 = lhs[offset]
  a.vaddss(xmm0, xmm0, dword_ptr(rcx, rax, 2)); // xmm0 = xmm0 + rhs[offset]
  a.lea(rdi, dword_ptr(rax, 1)); // size = offset + 1
  a.vmovss(dword_ptr(rsi, rax, 2), xmm0); // out[offset] = xmm0

  // Checks if loop is finished
  a.cmp(rax, r8); // if offset == size - 1, terminate
  a.jne(LoopInc);

  // Exit
  a.bind(Exit);
  a.ret();

  // Jits the code and stores in function
  addFunc fn;
  Error err = rt.add(&fn, &code);
  if (err) {
    std::cout << "Error while jitting!" << std::endl;
  }

  // Computes the add
  fn(array_size, out, lhs, rhs);

  // Debug prints the output
  std::cout << "Jitted function result: ";

  for (auto i = decltype(array_size){0}; i < array_size; ++i) {
    std::cout << out[i] << " ";
  }
  std::cout << std::endl;

  // Frees sample arrays
  std::free(out);
  std::free(lhs);
  std::free(rhs);

  return false;
}

bool cudaMergeNodeWithFusionGroup(const Node* const node, Node* fusion_group) {
  #if FUSER_DEBUG
    std::cout << "cudaMergeNodeWithFusionGroup" << std::endl;
  #endif // FUSER_DEBUG

  return false;
}

// Given a node and a fusion group, returns true if the node can be
// merged into the fusion group and false if it cannot.
// The fusion_group may be empty, specified by a nullptr.
// TODO: make this actually work (currently always returns false)
// TODO: validate that all inputs and outputs are on the same device (no multi-device fusion)

bool mergeNodeWithFusionGroup(const Node* const node, Node* fusion_group) {
  #if FUSER_DEBUG
    std::cout << "interface.cpp: addNode()" << std::endl;
    const auto is_valid = validateNode(node, true);
    std::cout << "is_valid: " << is_valid << std::endl;
  #else
    const auto is_valid = validateNode(node, false);
  #endif // FUSER_DEBUG

  if (!is_valid) {
    return false;
  }

  const auto& outputs = node->outputs();

  // Only single-output fusions (for now)
  if (outputs.size() != 1) {
    return false;
  }

  const auto& output = node->output();

  // Only tensor output fusions (for now)
  if (!(output->isCompleteTensor())) {
    return false;
  }

  const std::shared_ptr<c10::TensorType> out_tensor = output->type()->expect<TensorType>();
  const auto fusion_device = *(out_tensor->device());

  #if FUSER_DEBUG
    std::cout << "fusion device " << fusion_device << std::endl;
  #endif // FUSER_DEBUG

  if (fusion_device.type() == c10::kCPU) {
    #if FUSER_DEBUG
      std::cout << "fusing on CPU" << std::endl;
    #endif // FUSER_DEBUG
    return cpuMergeNodeWithFusionGroup(node, fusion_group);
  } else if (fusion_device.type() == c10::kCUDA) {
    #if FUSER_DEBUG
      std::cout << "fusing on CUDA" << std::endl;
    #endif // FUSER_DEBUG
    return cudaMergeNodeWithFusionGroup(node, fusion_group);
  } else {
    std::cout << "unknown fusion device: " << fusion_device << std::endl;
    return false;
  }

  return false;
}


// OLD STUFF BELOW HERE























int64_t registerFusion(const Node* fusion_group) {
  return fuser::registerFusion(fusion_group);
}

void runFusion(const int64_t key, Stack& stack) {
  const auto result = fuser::runFusion(key, stack);
  if (!result)
    fuser::runFallback(key, stack);
}

bool canFuseOnCPU() {
  return fuser::hasFusionBackend(at::DeviceType::CPU) &&
      detail::cpu_fuser_enabled;
}

bool canFuseOnGPU() {
  return fuser::hasFusionBackend(at::DeviceType::CUDA);
}

void overrideCanFuseOnCPU(bool value) {
  detail::cpu_fuser_enabled = value;
}

// Uses the above interface by stuffing the graph into a node and treating that
// node as a fusion group.
std::vector<at::Tensor> debugLaunchGraph(
    Graph& graph,
    at::ArrayRef<at::Tensor> inputs) {
  // Creates a fusion group node
  auto wrapper_graph = std::make_shared<Graph>();
  Node* fusion_group = wrapper_graph->insertNode(
      wrapper_graph->createWithSubgraph(prim::FusionGroup));
  fusion_group->g_(attr::Subgraph, graph.copy());
  for (size_t i = 0; i < graph.inputs().size(); ++i) {
    fusion_group->addInput(wrapper_graph->addInput());
  }
  for (size_t i = 0; i < graph.outputs().size(); ++i) {
    wrapper_graph->registerOutput(fusion_group->addOutput());
  }

  // Creates the stack, registers and runs the fusion
  Stack stack = fmap<IValue>(inputs);
  const auto key = fuser::registerFusion(fusion_group);
  fuser::runFusion(key, stack);
  return fmap(stack, [](const IValue& iv) { return iv.toTensor(); });
}

std::string debugGetFusedKernelCode(
    Graph& graph,
    at::ArrayRef<at::Tensor> inputs) {
  // Creates a fusion group node
  auto wrapper_graph = std::make_shared<Graph>();
  Node* fusion_group =
      wrapper_graph->insertNode(wrapper_graph->createWithSubgraph(prim::FusionGroup));
  fusion_group->g_(attr::Subgraph, graph.copy());
  for (size_t i = 0; i < graph.inputs().size(); ++i) {
    fusion_group->addInput(wrapper_graph->addInput());
  }
  for (size_t i = 0; i < graph.outputs().size(); ++i) {
    wrapper_graph->registerOutput(fusion_group->addOutput());
  }

  // Creates the stack, registers and runs the fusion
  Stack stack = fmap<IValue>(inputs);
  const auto key = fuser::registerFusion(fusion_group);

  std::string code;
  if (!fuser::runFusion(key, stack, &code)) {
    throw std::runtime_error("Could not run fusion for graph");
  }

  return code;
}

size_t nCompiledKernels() {
  return fuser::nCompiledKernels();
}

} // namespace jit
} // namespace torch
